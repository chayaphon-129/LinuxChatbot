import os
import streamlit as st
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv


st.set_page_config(page_title="Linux/Unix Command Chatbot ü§ñ", layout="centered") # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# 1. Load raw PDF(s)
DATA_PATH = "data/"

def load_pdf_files(data_path):
    """Loads PDF files from a directory."""
    loader = DirectoryLoader(data_path, glob='*.pdf', loader_cls=PyPDFLoader)
    documents = loader.load()
    print(f"Loaded {len(documents)} documents.")
    return documents

documents = load_pdf_files(DATA_PATH)

# 2. Create Chunks
def create_chunks(documents):
    """Splits documents into smaller text chunks."""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50) 
    text_chunks = text_splitter.split_documents(documents)
    print(f"Created {len(text_chunks)} text chunks.")
    return text_chunks

text_chunks = create_chunks(documents)

# 3. Create Vector Embeddings
def get_embedding_model():
    """Initializes the OpenAI embedding model."""
    embedding_model = OpenAIEmbeddings()
    return embedding_model

embedding_model = get_embedding_model()

# 4. ‡∏à‡∏±‡∏î‡πÄ‡∏Å‡πá‡∏ö Embeddings ‡πÉ‡∏ô FAISS
DB_FAISS_PATH = "vectorstore/db_faiss"
@st.cache_resource
def create_vectorstore(_text_chunks, _embedding_model, db_faiss_path):
    """Creates and saves the FAISS vectorstore."""
    db = FAISS.from_documents(_text_chunks, _embedding_model)
    db.save_local(db_faiss_path)
    print(f"Saved FAISS index to {db_faiss_path}")
    return db

db = create_vectorstore(text_chunks, embedding_model, DB_FAISS_PATH)

# 5. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LLM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö QA
def get_llm():
    """Initializes the OpenAI LLM."""
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5) 
    return llm

llm = get_llm()

# 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á RetrievalQA Chain
def create_qa_chain(llm, db):
    """Creates the RetrievalQA chain."""
    prompt_template = """‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô.
    ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö, ‡∏Ñ‡∏∏‡∏ì‡∏ï‡∏≠‡∏ö‡πÅ‡∏Ñ‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏ß‡πà‡∏≤ ‡∏Ç‡∏≠‡πÇ‡∏ó‡∏©‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö. ‡∏≠‡∏¢‡πà‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏≠‡∏á.

    Context: {context}
    Question: {question}

    Answer:
    """
    PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={'k': 3}), 
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

qa_chain = create_qa_chain(llm, db)

# 7. Chat with the LLM using Streamlit
def chat_with_llm():
    """Starts the chat interaction with the LLM using Streamlit."""
    #st.set_page_config(page_title="Linux/Unix Command Chatbot ü§ñ", layout="centered") # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏û‡∏à
    st.title("Command Line LinuxChatbot")
    st.markdown("‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö! üëã ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö **‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Linux ‡πÅ‡∏•‡∏∞ Unix** ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà")

    # Sidebar ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
    with st.sidebar:
        st.header("‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Chatbot ‡∏ô‡∏µ‡πâ")
        st.info(
            "Chatbot ‡∏ô‡∏µ‡πâ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á Linux ‡πÅ‡∏•‡∏∞ Unix "
            "‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡πÉ‡∏ô LLM."
        )
        st.markdown("**‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô:**")
        st.markdown("- ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ô‡∏ä‡πà‡∏≠‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á.")
        st.markdown("- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏£‡∏≤‡∏ö.")
        st.markdown("- ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô '‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á'.")

    # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
    if 'messages' not in st.session_state:
        st.session_state.messages = []

    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ä‡∏ó‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ‡∏ä‡πà‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    if prompt := st.chat_input("‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Linux ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏¢‡∏π‡∏ô‡∏¥‡∏Å‡∏ã‡πå/‡∏•‡∏¥‡∏ô‡∏∏‡∏Å‡∏ã‡πå:"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ QA Chain
        try:
            response = qa_chain.invoke({'query': prompt})
            result = response["result"]
            source_documents = response["source_documents"]

            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•
            full_response_content = result

            st.session_state.messages.append({"role": "assistant", "content": full_response_content})
            with st.chat_message("assistant"):
                st.markdown(full_response_content)

                # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å‡πÉ‡∏ô expander
                if source_documents:
                    with st.expander("üîó ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
                        for i, doc in enumerate(source_documents):
                            st.markdown(f"**‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà {i+1}**: {doc.metadata.get('source', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}")
                            st.code(doc.page_content, language='text')
                            st.markdown("---") # ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        except Exception as e:
            error_message = f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})

if __name__ == "__main__":
    chat_with_llm()

    
